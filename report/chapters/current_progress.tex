Development work on this project began with a prototype polygon-based renderer. I wrote this prototype to get a basic degree of familiarity with voxel rendering techniques, voxel memory management and 3D graphics programming. In this prototype, voxels were organised in a flat, fixed-size array in memory. Each voxel held 1 byte of data, a "density" value generated by a Perlin Noise function. I used the Marching Cubes surface polygonisation algorithm to transform this field of density values into a polygon mesh, which I then rendered with OpenGL. I utilised the GLFW3 \autocite{lowy2019glfw} windowing library and the GLAD \autocite{herberth2019glad} OpenGL extension loading library for this project. Both of these libraries use permissive, open-source software licenses ("zlib" license and MIT license, respectively).

Although I eventually decided this prototype design was too limited to use as the main basis for this project, it did allow me to write all of the 3D camera and mathematics code required for the viewer application. Since the viewer application is not intended for use by third party developers, I considered it acceptable to use GLFW3 and GLAD to handle the window creation and input code. 

Next, I began implementing a Sparse Voxel Octree data structure for scene storage, following the theory laid down by Laine and Karras. I also incorporated the windowing, camera and input code from the previously mentioned prototype into the \texttt{sabre\_viewer} application. At this early stage, work on the SVO library and the viewer application occurred largely in parallel.

Having decided to progress with a volumetric rendering approach, I first considered using an OpenGL fragment shader for the ray-casting. Fragment shaders are a type of GPU program that operate on every \textit{fragment} (analoguous to pixels) of object the GPU is drawing.

\begin{figure}[ht]
    \centering
    \includesvg[width=100px]{graphics/screenquad.svg}
    \caption{All of the rendering in \textit{Sabre} is done with just two triangles. This shape covers the entire screen, and projects a texture created by a compute shader}
    \label{fig:screen_quad}
\end{figure}

To begin rendering, I created a very simple flat surface polygon, formed from two triangles (see figure \ref{fig:screen_quad}). I then set up a fragment shader to begin drawing each fragment of this flat object. By having the quad cover the entire viewport, the fragment shader executes once for each individual pixel of the output image. 

Shortly after beginning work on the fragment shader, I decided to switch to a compute shader based approach. Compute shaders are another type of GPU program, more general than fragment shaders. Compute shaders are executed many times in a single frame, exploiting the highly parallel nature of GPU hardware. I decided to use compute shaders for two reasons: firstly, to learn about GPGPU programming, an important topic in high-performance software engineering, and because there is some evidence that compute shaders can be faster at ray-casting than fragment shaders \autocite{francisco2017comparison}. As, in OpenGL, fragment shaders and compute shaders both use the GLSL programming language, I was able to transform what existing ray-casting code I had into a compute shader with little effort. 

The compute shader renderer currently operates by casting a ray through the sparse voxel octree data structure for every pixel of the output image. Depending on whether or not the ray hits a leaf voxel, a particular color is written to an output texture. This texture is then projected onto the screen quad. 

Throughout development, I used the same implicit surface function detailed in section \ref{sec:poly_renderer_research} to generate a sparse voxel octree data structure that describes a sphere. The ability to easily generate voxel fields out of mathematical functions is enormously helpful when debugging as no external assets need to be imported to produce test models.
